name: ML Model CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    paths: 
      - 'ml/**'
      - '.github/workflows/ml-ci-cd.yml'
  pull_request:
    branches: [ main ]
    paths: 
      - 'ml/**'
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Model to deploy'
        required: true
        default: 'anomaly_detection'
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production
      force_retrain:
        description: 'Force model retraining'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.9'
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  MLFLOW_S3_ENDPOINT_URL: ${{ secrets.MLFLOW_S3_ENDPOINT_URL }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

jobs:
  test:
    name: Test ML Pipeline
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./ml
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_ml
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üèó Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-mock

      - name: üîç Run linting
        run: |
          flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: üè∑ Run type checking
        run: mypy src/ --ignore-missing-imports

      - name: üß™ Run unit tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_ml
          REDIS_URL: redis://localhost:6379/0
        run: |
          pytest tests/ -v --cov=src --cov-report=xml --cov-report=html

      - name: üìä Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          flags: ml
          directory: ./ml

  data-validation:
    name: Data Quality Validation
    runs-on: ubuntu-latest
    needs: test
    defaults:
      run:
        working-directory: ./ml
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üèó Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install dependencies
        run: |
          pip install -r requirements.txt

      - name: üîç Run data validation
        run: |
          python scripts/validate_data.py --data-path data/sample/ --config config/data_validation.yaml

      - name: üìà Generate data quality report
        run: |
          python scripts/data_quality_report.py --output reports/data_quality.html

      - name: üìé Upload data quality report
        uses: actions/upload-artifact@v3
        with:
          name: data-quality-report
          path: ml/reports/data_quality.html

  model-training:
    name: Model Training & Evaluation
    runs-on: ubuntu-latest
    needs: [test, data-validation]
    if: github.ref == 'refs/heads/main' || github.event.inputs.force_retrain == 'true'
    defaults:
      run:
        working-directory: ./ml
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: ml_training
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üèó Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install dependencies
        run: |
          pip install -r requirements.txt

      - name: üéØ Train models
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/ml_training
          REDIS_URL: redis://localhost:6379/0
        run: |
          python scripts/train_models.py \
            --config config/training_config.yaml \
            --experiment-name "ci-cd-training" \
            --models anomaly_detection,performance_prediction,cost_forecasting

      - name: üìä Evaluate models
        run: |
          python scripts/evaluate_models.py \
            --experiment-name "ci-cd-training" \
            --output reports/model_evaluation.json

      - name: üèÜ Select best models
        run: |
          python scripts/model_selection.py \
            --evaluation-report reports/model_evaluation.json \
            --output reports/selected_models.json

      - name: üìé Upload training artifacts
        uses: actions/upload-artifact@v3
        with:
          name: training-artifacts
          path: |
            ml/reports/model_evaluation.json
            ml/reports/selected_models.json

  model-validation:
    name: Model Validation & Testing
    runs-on: ubuntu-latest
    needs: model-training
    defaults:
      run:
        working-directory: ./ml
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üèó Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install dependencies
        run: |
          pip install -r requirements.txt

      - name: üì• Download training artifacts
        uses: actions/download-artifact@v3
        with:
          name: training-artifacts
          path: ml/reports/

      - name: üß™ Run model tests
        run: |
          python scripts/test_models.py \
            --models-config reports/selected_models.json \
            --test-data data/test/ \
            --output reports/model_tests.json

      - name: üîç Model bias detection
        run: |
          python scripts/bias_detection.py \
            --models-config reports/selected_models.json \
            --test-data data/test/ \
            --output reports/bias_analysis.json

      - name: üìà Performance benchmarking
        run: |
          python scripts/benchmark_models.py \
            --models-config reports/selected_models.json \
            --benchmark-data data/benchmark/ \
            --output reports/performance_benchmark.json

      - name: üìé Upload validation artifacts
        uses: actions/upload-artifact@v3
        with:
          name: validation-artifacts
          path: |
            ml/reports/model_tests.json
            ml/reports/bias_analysis.json
            ml/reports/performance_benchmark.json

  security-scan:
    name: Security & Vulnerability Scan
    runs-on: ubuntu-latest
    needs: test
    defaults:
      run:
        working-directory: ./ml
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üèó Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üîí Run safety check
        run: |
          pip install safety
          safety check --json --output safety-report.json || true

      - name: üîç Run bandit security scan
        run: |
          pip install bandit
          bandit -r src/ -f json -o bandit-report.json || true

      - name: üìé Upload security reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            ml/safety-report.json
            ml/bandit-report.json

  build-inference-api:
    name: Build Inference API
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üèó Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: üîë Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: üè∑ Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}/ml-inference-api
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: üê≥ Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: ./ml
          file: ./ml/Dockerfile.inference
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [model-validation, build-inference-api]
    if: github.ref == 'refs/heads/develop' || (github.event.inputs.environment == 'staging')
    environment: staging
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üì• Download validation artifacts
        uses: actions/download-artifact@v3
        with:
          name: validation-artifacts
          path: ./reports/

      - name: üöÄ Deploy to Staging
        env:
          KUBE_CONFIG: ${{ secrets.STAGING_KUBE_CONFIG }}
          STAGING_NAMESPACE: opssight-ml-staging
        run: |
          echo "$KUBE_CONFIG" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
          
          # Update model registry with staging models
          python ml/scripts/deploy_models.py \
            --environment staging \
            --models-config reports/selected_models.json \
            --namespace $STAGING_NAMESPACE
          
          # Deploy inference API
          helm upgrade --install ml-inference-api \
            helm/ml-inference-api \
            --namespace $STAGING_NAMESPACE \
            --set image.tag=${{ github.sha }} \
            --set environment=staging \
            --wait

      - name: üß™ Run staging tests
        run: |
          python ml/scripts/staging_tests.py \
            --api-url ${{ secrets.STAGING_API_URL }} \
            --models-config reports/selected_models.json

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.ref == 'refs/heads/main' || (github.event.inputs.environment == 'production')
    environment: production
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üì• Download validation artifacts
        uses: actions/download-artifact@v3
        with:
          name: validation-artifacts
          path: ./reports/

      - name: üéØ Production Deployment Approval
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: ml-team,devops-team
          minimum-approvals: 2
          issue-title: "Deploy ML Models to Production"
          issue-body: |
            Please review the ML model deployment to production.
            
            **Models**: ${{ github.event.inputs.model_name || 'All models' }}
            **Commit**: ${{ github.sha }}
            **Branch**: ${{ github.ref }}
            
            **Validation Results**: See attached artifacts
            
            Please approve this deployment if the models meet production requirements.

      - name: üöÄ Deploy to Production
        env:
          KUBE_CONFIG: ${{ secrets.PRODUCTION_KUBE_CONFIG }}
          PRODUCTION_NAMESPACE: opssight-ml-production
        run: |
          echo "$KUBE_CONFIG" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
          
          # Update model registry with production models
          python ml/scripts/deploy_models.py \
            --environment production \
            --models-config reports/selected_models.json \
            --namespace $PRODUCTION_NAMESPACE \
            --enable-canary
          
          # Deploy inference API with blue-green deployment
          helm upgrade --install ml-inference-api \
            helm/ml-inference-api \
            --namespace $PRODUCTION_NAMESPACE \
            --set image.tag=${{ github.sha }} \
            --set environment=production \
            --set deployment.strategy=blue-green \
            --wait

      - name: üîç Production health check
        run: |
          python ml/scripts/production_health_check.py \
            --api-url ${{ secrets.PRODUCTION_API_URL }} \
            --models-config reports/selected_models.json \
            --timeout 300

      - name: üìä Setup monitoring
        run: |
          python ml/scripts/setup_monitoring.py \
            --environment production \
            --models-config reports/selected_models.json \
            --prometheus-url ${{ secrets.PROMETHEUS_URL }}

  performance-monitoring:
    name: Performance Monitoring
    runs-on: ubuntu-latest
    needs: deploy-production
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: üèó Setup repo
        uses: actions/checkout@v4

      - name: üèó Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üì¶ Install dependencies
        run: |
          pip install -r ml/requirements.txt

      - name: üìà Monitor model performance
        run: |
          python ml/scripts/monitor_models.py \
            --environment production \
            --duration 3600 \
            --alert-threshold 0.1 \
            --output reports/monitoring_report.json

      - name: üö® Create performance alerts
        if: failure()
        run: |
          python ml/scripts/create_alerts.py \
            --report reports/monitoring_report.json \
            --webhook ${{ secrets.SLACK_WEBHOOK_URL }}

  cleanup:
    name: Cleanup Resources
    runs-on: ubuntu-latest
    needs: [deploy-production, performance-monitoring]
    if: always()
    
    steps:
      - name: üßπ Cleanup old model versions
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        run: |
          python ml/scripts/cleanup_models.py \
            --keep-versions 5 \
            --archive-threshold 30d

      - name: üóë Cleanup old artifacts
        run: |
          # Cleanup old Docker images
          docker image prune -af --filter "until=168h"
          
          # Cleanup old logs and reports (if stored locally)
          find . -name "*.log" -mtime +7 -delete
          find . -name "*_report.json" -mtime +30 -delete